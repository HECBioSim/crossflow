{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossflow 101\n",
    "An introduction to the fundamentals of Crossflow\n",
    "\n",
    "Workflows are a common feature of much computational science. In a workflow, the work to be done requires more than one piece of software, and the output from one becomes the input to the next, in some form of chain. Classically one would write some sort of bash script or similar to do the job, e.g.:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "input_file=input.dat\n",
    "intermediate_file=intermediate.dat\n",
    "result_file=result.dat\n",
    "\n",
    "executable1 -i $input_file -o $intermediate_file\n",
    "executable2 -i $intermediate_file -o $result_file\n",
    "\n",
    "```\n",
    "This is OK for basic use but:\n",
    "* what if your workflow has loops, conditional executions, etc?\n",
    "* what happens if you want to do things at scale?\n",
    "\n",
    "Crossflow is designed to make this easier. Key points are:\n",
    "\n",
    "1. The workflow becomes a Python program, and can make use of all programming workflow constructs (loops, if/then/else, etc.)\n",
    "2. To do this, it provides a simple approach to turning command line tools into Python functions - this is `crossflow.tasks`.\n",
    "3. It provides a way to hand the processing of individual workflow steps out to a distributed cluster of workers - this is `crossflow.clients`.\n",
    "\n",
    "Here we look at each of these components in turn.\n",
    "\n",
    "--------------------\n",
    "## Crossflow Tasks\n",
    "\n",
    "The `crossflow.tasks` subpackage provides methods to turn tools that would usually be used via the command line into Python functions. The basic concept is that a tool that is used from the commmand line something like:\n",
    "```bash\n",
    "my_tool -i input.dat -o output.dat\n",
    "```\n",
    "becomes, in Python:\n",
    "```\n",
    "output = my_tool_task('input.dat')\n",
    "```\n",
    "`\n",
    "Where my_tool_task` is a `crossflow.SubprocessTask` for `my_tool` and `output` is a `crossflow.FileHandle`, which behaves much like a Python `Path` object (see [here](https://docs.python.org/3/library/pathlib.html)).\n",
    "\n",
    "### Creating a crossflow.SubprocessTask\n",
    "\n",
    "This is a three step process:\n",
    "\n",
    "1. The task is created on the basis of a `template`, a string with a generalised version of the command you wish to execute.\n",
    "2. The inputs for the task are specified.\n",
    "3. The outputs from the task are specified.\n",
    "\n",
    "Thus:\n",
    "```python\n",
    "my_tool_task = crossflow.tasks.SubprocessTask('my_tool -i x.in -o x.out')\n",
    "my_tool_task.set_inputs(['x.in'])\n",
    "my_tool_task.set_outputs(['x.out'])\n",
    "```\n",
    "Note that the names of files used in the template string are arbitrary, 'my_tool -i a -o b' would do just as well, as long as the corresponding names ('a', 'b') were used in .set_inputs() and .set_outputs().\n",
    "\n",
    "If the tool takes multiple files as inputs, and/or produces multiple output files, the process is the same:\n",
    "```python\n",
    "my_othertool_task = crossflow.tasks.SubprocessTask('my_othertool -x x.in -y y.in -o x.out -l logfile')\n",
    "my_othertool_task.set_inputs(['x.in', 'y.in'])\n",
    "my_othertool_task.set_outputs(['x.out', 'logfile'])\n",
    "```\n",
    "\n",
    "There is no restriction on the order that inputs and outputs are specified in the template string, but the resulting task will expect its inputs to be provided in the order they are given in .set_inputs() and the tuple of outputs the task produces will be in the order they are specified in .set_outputs().\n",
    "\n",
    "For more advanced aspects of `SubprocessTask` creation, see elsewhere.\n",
    "\n",
    "### Running a crossflow.SubprocessTask\n",
    "Although it is primarily expected that tasks will be run via a `crossflow.Client` (see below), they can also be executed directly:\n",
    "```python\n",
    "output, logfile = my_othertool(x, y)\n",
    "```\n",
    "As explained above, `output` and `logfile` will be 'Path-like' objects (but with more limited functionality than real `Path` objects). So to save the output to a local file:\n",
    "```python\n",
    "output.save('output.dat')\n",
    "```\n",
    "Or to look at the contents of the logfile directly:\n",
    "```python\n",
    "print(logfile.read_text())\n",
    "```\n",
    "\n",
    "--------------------\n",
    "## Crossflow Clients\n",
    "The `crossflow.clients` sub-package provides a Client through which one can execute tasks on distributed resources. At its heart a `crossflow.clients.Client()` is a [dask.distributed](https://distributed.dask.org/en/latest/) client, and new users are strongly encouraged to read the documentation there to understand how Crossflow works.\n",
    "\n",
    "### Creating a crossflow.Client\n",
    "\n",
    "A Crossflow client provides access to a cluster of workers. These may be remote machines, or a set of worker processes on the current compute resource (see the dask documentation for more details). The cluster may be already up and running, in which case the crossflow.Client just needs to know where it is (the address of its scheduler):\n",
    "\n",
    "```python\n",
    "my_client = crossflow.clients.Client(scheduler_file='scheduler.json')\n",
    "```\n",
    "\n",
    "Alternatively (typically for testing purposes), a local cluster may be created on the fly, to serve the Client:\n",
    "```python\n",
    "my_client = crossflow.clients.Client()\n",
    "```\n",
    "\n",
    "More generally, there are ways of creating compatible `cluster` objects on a wide variety of resources from clouds to HPC systems. See for example [dask jobqueue](https://jobqueue.dask.org/en/latest/) and [dask kubernetes](https://kubernetes.dask.org/en/latest/).\n",
    "\n",
    "\n",
    "### Using a crossflow.Client\n",
    "\n",
    "A crossflow.Task is sent to a crossflow.Client for execution using the client's .submit() or .map() method.\n",
    "\n",
    "\n",
    "#### Running a single job:\n",
    "```python\n",
    "output_future, logfile_future = my_client.submit(my_othertool_task, x, y)\n",
    "```\n",
    "Compare with the interactive version above. The outputs (output_future, logfile_future) are now Futures - again, see the dask documentation for more detail, but also notice the difference: dask's .submit() method always returns a single Future, while crossflow's one returns one Future per expected output.\n",
    "\n",
    "#### Running a set of jobs in parallel:\n",
    "```python\n",
    "xs = [x1, x2, x3, x4]\n",
    "ys = [y1, y2, y3, y4]\n",
    "output_futures, logfile_futures = my_client.map(my_othertool_task, xs, ys)\n",
    "```\n",
    "In this case the .map() method returns lists of Futures. The individual jobs are scheduled to the workers in the compute cluster in whatever way is most efficient, if there are enough of them to run all four jobs in parallel, they will.\n",
    "\n",
    "-------------\n",
    "## A simple demonstration\n",
    "\n",
    "Here we create a `SubprocessTask` to reverse the order of the lines in a file, submit the job to a local `Client`, and then retrieve and view the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow import clients, tasks\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a short text file:\n",
    "here = Path('.')\n",
    "inp_file = here / 'input.txt'\n",
    "with inp_file.open('w') as f:\n",
    "    for i in range(10):\n",
    "        f.write('line {}\\n'.format(i))\n",
    "\n",
    "print('Original file:')\n",
    "print(inp_file.read_text())\n",
    "\n",
    "# Create a SubprocessTask that will reverse the lines in a file:\n",
    "reverser = tasks.SubprocessTask('rev input > output')\n",
    "reverser.set_inputs(['input'])\n",
    "reverser.set_outputs(['output'])\n",
    "\n",
    "\n",
    "# Create a local client to run the job, and submit it:\n",
    "client = clients.Client()\n",
    "output = client.submit(reverser, inp_file)\n",
    "\n",
    "# output is a Future; collect its result(), which is a 'Path-like' \n",
    "# FileHandle, list its contents, then save to a file:\n",
    "\n",
    "print('New file:')\n",
    "print(output.result().read_text())\n",
    "output.result().save(here / 'output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with errors\n",
    "\n",
    "If you call the `.result()` method on a future that comes from a task that has failed, this will raise an exception. In general this is a bit messy, so it's good practice write your code to catch these.\n",
    "\n",
    "One option is to wrap tasks in try/except blocks, another to test the value of the `.status` attribute of the future, once the task has completed (using `distributed`'s `wait` function).\n",
    "\n",
    "e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_command = tasks.SubprocessTask('foo input > output')\n",
    "bad_command.set_inputs(['input'])\n",
    "bad_command.set_outputs(['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The messy way:\n",
    "output = client.submit(bad_command, inp_file)\n",
    "print(output.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clean way #1:\n",
    "output = client.submit(bad_command, inp_file)\n",
    "try:\n",
    "    print(output.result())\n",
    "except:\n",
    "    print(output.exception())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clean way #2:\n",
    "from distributed import wait\n",
    "\n",
    "output = client.submit(bad_command, inp_file)\n",
    "wait(output)\n",
    "if output.status != 'error':\n",
    "    print(output.result())\n",
    "else:\n",
    "    print('there was an error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "taskspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
