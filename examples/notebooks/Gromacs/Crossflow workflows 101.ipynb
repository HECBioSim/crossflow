{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An introduction to Crossflow workflows\n",
    "\n",
    "In this notebook you will see how a simple MD simulation job can be converted from its normal command-line form into a Python function using tools in *Crossflow*.\n",
    "\n",
    "Then you will see how it's easy to chain jobs together to create a workflow.\n",
    "\n",
    "The notebook assumes you have a basic knowledge of *Gromacs*, and the Python package *MDTraj*, and that both of these are installed on the computer you are running this notebook on.\n",
    "\n",
    "In addition it's assumed you have installed *Crossflow* (e.g. `pip install crossflow`).\n",
    "\n",
    "----\n",
    "\n",
    "### Part 1: running jobs the conventional way\n",
    "Have a look at the contents of this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bpti-em.edr\t    bpti_min_1.gro\t\t     em.log\n",
      "'#bpti-em.edr.1#'   bpti_min_2.gro\t\t     em.mdp\n",
      " bpti-em.gro\t    bpti_min_3.gro\t\t     final_coordinates.gro\n",
      "'#bpti-em.gro.1#'   bpti_min_4.gro\t\t     max_rmsd.gro\n",
      " bpti-em.log\t    bpti_min_5.gro\t\t     mdout.mdp\n",
      "'#bpti-em.log.1#'   bpti_min_6.gro\t\t     nvt.log\n",
      " bpti-em.tpr\t    bpti_min_7.gro\t\t     nvt.mdp\n",
      "'#bpti-em.tpr.1#'   bpti_min_8.gro\t\t     provision.dat\n",
      " bpti-em.trr\t    bpti_min_9.gro\t\t     README.md\n",
      "'#bpti-em.trr.1#'   bpti.top\t\t\t     test.gro\n",
      " bpti.gro\t   'Crossflow 201.ipynb'\t     test.log\n",
      " bpti_min_0.gro    'Crossflow workflows 101.ipynb'\n",
      " bpti_min_10.gro    dask-worker-space\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "\n",
    "    Crossflow workflows 101.ipynb : This notebook\n",
    "    bpti.gro                      : Coordinates for BPTI in Gromacs .gro format\n",
    "    bpti.top                      : Gromacs topology file for BPTI\n",
    "    em.mdp                        : A Gromacs .mdp file defining an energy minimisation job\n",
    "    nvt.mdp                       : a Gromacs .mdp file defining a short NVT MD simulation\n",
    "    \n",
    "Let's begin by running the energy minimisation job interactively in the conventional way. \n",
    "\n",
    "First we run grompp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      :-) GROMACS - gmx grompp, 2022.2 (-:\n",
      "\n",
      "Executable:   /usr/remote/gromacs/2022.2/bin/gmx\n",
      "Data prefix:  /usr/remote/gromacs/2022.2\n",
      "Working dir:  /users/charlie/crossflow/examples/Notebooks/Gromacs\n",
      "Command line:\n",
      "  gmx grompp -f em.mdp -c bpti.gro -p bpti.top -o bpti-em.tpr\n",
      "\n",
      "Ignoring obsolete mdp entry 'ns_type'\n",
      "\n",
      "NOTE 1 [file em.mdp]:\n",
      "  With Verlet lists the optimal nstlist is >= 10, with GPUs >= 20. Note\n",
      "  that with the Verlet scheme, nstlist has no effect on the accuracy of\n",
      "  your simulation.\n",
      "\n",
      "Setting the LD random seed to -38023\n",
      "\n",
      "Generated 2145 of the 2145 non-bonded parameter combinations\n",
      "Generating 1-4 interactions: fudge = 0.5\n",
      "\n",
      "Generated 2145 of the 2145 1-4 parameter combinations\n",
      "\n",
      "Excluding 3 bonded neighbours molecule type 'Protein'\n",
      "\n",
      "Excluding 2 bonded neighbours molecule type 'SOL'\n",
      "\n",
      "Excluding 1 bonded neighbours molecule type 'CL'\n",
      "Analysing residue names:\n",
      "There are:    58    Protein residues\n",
      "There are:  6541      Water residues\n",
      "There are:     6        Ion residues\n",
      "Analysing Protein...\n",
      "Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...\n",
      "Number of degrees of freedom in T-Coupling group rest is 41937.00\n",
      "\n",
      "The largest distance between excluded atoms is 0.423 nm\n",
      "Calculating fourier grid dimensions for X Y Z\n",
      "Using a fourier grid of 52x52x52, spacing 0.114 0.114 0.114\n",
      "\n",
      "Estimate for the relative computational load of the PME mesh part: 0.22\n",
      "\n",
      "This run will generate roughly 2 Mb of data\n",
      "\n",
      "There was 1 note\n",
      "\n",
      "Back Off! I just backed up bpti-em.tpr to ./#bpti-em.tpr.2#\n",
      "\n",
      "GROMACS reminds you: \"We are perhaps not far removed from the time when we shall be able to submit the bulk of chemical phenomena to calculation.\" (Joseph Gay-Lussac, 1808)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gmx grompp -f em.mdp -c bpti.gro -p bpti.top -o bpti-em.tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything there went as expected, now we can run the energy minimisation itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      :-) GROMACS - gmx mdrun, 2022.2 (-:\n",
      "\n",
      "Executable:   /usr/remote/gromacs/2022.2/bin/gmx\n",
      "Data prefix:  /usr/remote/gromacs/2022.2\n",
      "Working dir:  /users/charlie/crossflow/examples/Notebooks/Gromacs\n",
      "Command line:\n",
      "  gmx mdrun -s bpti-em.tpr -c bpti-em.gro -g bpti-em.log -o bpti-em.trr -e bpti-em.edr\n",
      "\n",
      "\n",
      "Back Off! I just backed up bpti-em.log to ./#bpti-em.log.2#\n",
      "Compiled SIMD: AVX_256, but for this host/run AVX2_256 might be better (see\n",
      "log).\n",
      "Reading file bpti-em.tpr, VERSION 2022.2 (single precision)\n",
      "1 GPU selected for this run.\n",
      "Mapping of GPU IDs to the 1 GPU task in the 1 rank on this node:\n",
      "  PP:0\n",
      "PP tasks will do (non-perturbed) short-ranged interactions on the GPU\n",
      "PP task will update and constrain coordinates on the CPU\n",
      "Using 1 MPI thread\n",
      "Using 8 OpenMP threads \n",
      "\n",
      "\n",
      "Back Off! I just backed up bpti-em.trr to ./#bpti-em.trr.2#\n",
      "\n",
      "Back Off! I just backed up bpti-em.edr to ./#bpti-em.edr.2#\n",
      "\n",
      "Steepest Descents:\n",
      "   Tolerance (Fmax)   =  1.00000e+03\n",
      "   Number of steps    =         1000\n",
      "\n",
      "writing lowest energy coordinates.\n",
      "\n",
      "Back Off! I just backed up bpti-em.gro to ./#bpti-em.gro.2#\n",
      "\n",
      "Steepest Descents converged to Fmax < 1000 in 178 steps\n",
      "Potential Energy  = -3.1763678e+05\n",
      "Maximum force     =  9.3962616e+02 on atom 48\n",
      "Norm of force     =  3.9200161e+01\n",
      "\n",
      "GROMACS reminds you: \"FORTRAN, the infantile disorder, by now nearly 20 years old, is hopelessly inadequate for whatever computer application you have in mind today: it is now too clumsy, too risky, and too expensive to use.\" (Edsger Dijkstra, 1970)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gmx mdrun -s bpti-em.tpr -c bpti-em.gro -g bpti-em.log -o bpti-em.trr -e bpti-em.edr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the job completed without errors, you should see the output files in the current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bpti-em.edr\t   '#bpti-em.trr.2#'  'Crossflow 201.ipynb'\n",
      "'#bpti-em.edr.1#'   bpti.gro\t      'Crossflow workflows 101.ipynb'\n",
      "'#bpti-em.edr.2#'   bpti_min_0.gro     dask-worker-space\n",
      " bpti-em.gro\t    bpti_min_10.gro    em.log\n",
      "'#bpti-em.gro.1#'   bpti_min_1.gro     em.mdp\n",
      "'#bpti-em.gro.2#'   bpti_min_2.gro     final_coordinates.gro\n",
      " bpti-em.log\t    bpti_min_3.gro     max_rmsd.gro\n",
      "'#bpti-em.log.1#'   bpti_min_4.gro     mdout.mdp\n",
      "'#bpti-em.log.2#'   bpti_min_5.gro     nvt.log\n",
      " bpti-em.tpr\t    bpti_min_6.gro     nvt.mdp\n",
      "'#bpti-em.tpr.1#'   bpti_min_7.gro     provision.dat\n",
      "'#bpti-em.tpr.2#'   bpti_min_8.gro     README.md\n",
      " bpti-em.trr\t    bpti_min_9.gro     test.gro\n",
      "'#bpti-em.trr.1#'   bpti.top\t       test.log\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat bpti-em.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Turning this into Python\n",
    "\n",
    "OK. Now you will see how you can turn the energy minimisation job from something you run on the command line (in this situation, within a Jupyter notebook, by using the \"!\" special command) into a pure Python function.\n",
    "\n",
    "The function will take a .tpr file as the input, and return the .gro and .log files when the job completes. For now, you can assume you are not that bothered about what's in the .edr and .trr files.\n",
    "\n",
    "So your aim is something like this:\n",
    "\n",
    "    grofile, logfile = md(tprfile)\n",
    "    \n",
    "---\n",
    "\n",
    "Begin by importing the required submodules from *Crossflow*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow import filehandling, tasks, clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you create the function, which in *Crossflow* is called a **Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrun = tasks.SubprocessTask('gmx mdrun -s x.tpr -c x.gro -g x.log -e x.edr -o x.trr -ntmpi 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that creating the task involves providing a template for the command you want to run. The names of the files in the template are completely up to you (e.g. you could use \"system.tpr\", etc. instead of \"x.tpr\") - but in general make sure the filenames have the appropriate extensions.\n",
    "\n",
    "---\n",
    "\n",
    "Now you have to tell the task what files are inputs, and what are outputs. To do this you pass *lists* of strings that correspond to the filenames in the template above. \n",
    "\n",
    "**NB:** the order of the strings in the inputs list defines the order that input variables will be passed to the task, and the order of the strings in the output list defines the order that the outputs from the function will appear in.\n",
    "\n",
    "**NB2:** You only get the outputs you ask for. So although the job is going to produce trajectory (x.trr) and energy(x.edr) files, you are not going to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the task the signature: grofile, logfile = md(tprfile)\n",
    "md.set_inputs(['x.tpr'])\n",
    "md.set_outputs(['x.gro', 'x.log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Crossflow* runs your job on a **cluster**. This can be anything from a number of processes/threads on your current computer, to an HPC cluster or a set of resources in the cloud. The way you create the cluster depends on which of these you choose, but once it's up and running the way you run jbs on it via crossflow is always the same.\n",
    "\n",
    "In this case we are going to run the jobs on the local computer, which we assume can really only run one MD job at a time, so we create a **LocalCluster** with one **worker**. The code to do this comes from the `dask.distributed` package which underpins *Crossflow* and which you therefore have already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 15:24:58,738 - distributed.diskutils - INFO - Found stale lock file and directory '/users/charlie/crossflow/examples/Notebooks/Gromacs/dask-worker-space/worker-ogvg37_o', purging\n"
     ]
    }
   ],
   "source": [
    "from distributed import LocalCluster\n",
    "cluster = LocalCluster(n_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a *Crossflow* **client** to serve your cluster, and then submit the job (the function and its arguments) to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = clients.Client(cluster)\n",
    "fh = filehandling.FileHandler()\n",
    "tprfile = fh.load('bpti-em.tpr')\n",
    "grofile, logfile = client.submit(mdrun, tprfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job is now submitted for computation, which takes place in the background. The output variables `grofile` and `logfile` are `Futures`, whose final values are obtained by calling their `result()` methods. Use this to save the outputs to local files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: finished, type: crossflow.filehandling.FileHandle, key: lambda-9d021a68afb22c0a31573eb863c0aa9a>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.gro'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(grofile)\n",
    "logfile.result().save('test.log')\n",
    "grofile.result().save('test.gro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat test.gro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: A workflow\n",
    "\n",
    "Let's make a workflow that runs a grompp job, then immediately the md (or energy minimisation) job.\n",
    "\n",
    "You already have a task that can run *mdrun*, but you need to build one to run *grompp*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a task with the signature: tprfile = grompp(mdpfile, grofile, topfile):\n",
    "grompp = tasks.SubprocessTask('gmx grompp -f x.mdp -c x.gro -p x.top -o x.tpr -maxwarn 1')\n",
    "grompp.set_inputs(['x.mdp', 'x.gro', 'x.top'])\n",
    "grompp.set_outputs(['x.tpr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables from the required input files:\n",
    "emfile = 'em.mdp'\n",
    "start_crds = 'bpti.gro'\n",
    "topfile = 'bpti.top'\n",
    "# Run the job:\n",
    "em_tprfile = client.submit(grompp, emfile, start_crds, topfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from this task should be ready for use in the mdrun task - let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'em.log'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the energy minimisation:\n",
    "em_crds, em_logfile = client.submit(mdrun, em_tprfile)\n",
    "em_logfile.result().save('em.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat em.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You may have spotted that the first time you ran `client submit(mdrun, ...)` the argument was a string (the name of a tprfile), but the second time it was a *future* that points at a tprfile object. That's fine - the client.submit() function works all that out for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Exercise - a bigger workflow\n",
    "\n",
    "Now we add the second simulation stage - the NVT MD - into your workflow.\n",
    "\n",
    "Notice you don't need to make any new tasks - you can re-use the ones you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvt.log'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A workflow that runs an energy minimisation and then an NVT MD simulation\n",
    "em_tprfile = client.submit(grompp, emfile, start_crds, topfile)\n",
    "em_crds, em_logfile = client.submit(mdrun, em_tprfile)\n",
    "nvtfile = 'nvt.mdp'\n",
    "nvt_tprfile = client.submit(grompp, nvtfile, em_crds, topfile)\n",
    "nvt_crds, nvt_logfile = client.submit(mdrun, nvt_tprfile)\n",
    "nvt_logfile.result().save('nvt.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: A better workflow\n",
    "\n",
    "Let's improve the workflow. Firstly, it would be nice if the NVT simulation job also returned the trajectory file. You don't want this for the EM job, so what that means is that you need to make a second mdrun-type task. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrun_with_traj = mdrun.copy()\n",
    "mdrun_with_traj.set_outputs(['x.gro', 'x.log', 'x.trr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The copy() convenience method saves you having to rewrite the task from scratch, if  it's just a tweak on an existing one. But it is also neccessary if you want to tweak a task that has already been used in a client.submit() call (if you want to understand why, see the dask.distributed documentation about 'pure' vs. 'impure' functions).\n",
    "\n",
    "Secondly, notice that both grompp jobs in the workflow above take the same topology file as an argument - in effect, it's a constant. In such cases, you can define it as such at the time you create the task, and then you don't have to include it in the list of arguments when you call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvt.log'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grompp2 = grompp.copy()\n",
    "grompp2.set_constant('x.top', topfile)\n",
    "# Now the new improved workflow:\n",
    "em_tprfile = client.submit(grompp2, emfile, start_crds) # no need to specify a topfile here\n",
    "em_crds, em_logfile = client.submit(mdrun, em_tprfile)\n",
    "nvt_tprfile = client.submit(grompp2, nvtfile, em_crds)\n",
    "nvt_crds, nvt_logfile, nvt_traj = client.submit(mdrun_with_traj, nvt_tprfile)\n",
    "nvt_logfile.result().save('nvt.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: interfacing with more Python\n",
    "\n",
    "At this stage you may be thinking \"OK - but nothing here I couldn't do with a bash script\". The power of the workflow comes when you interface your new pythonized-MD functions with other Python tools.\n",
    "\n",
    "Let's make use of the *MDTraj* package for analysis of MD trajectory data. You will use it to calculate the RMSD of the trajectory frames from the starting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mdtraj.Trajectory with 11 frames, 20521 atoms, 6605 residues, and unitcells>\n",
      "[0.         0.08701991 0.09370182 0.1102126  0.11426546 0.11102293\n",
      " 0.1252876  0.12361396 0.13193893 0.12360178 0.13172925]\n"
     ]
    }
   ],
   "source": [
    "import mdtraj as mdt\n",
    "traj = mdt.load(nvt_traj.result(), top=start_crds)\n",
    "print(traj)\n",
    "# Calculate the rmsd of each frame from the first:\n",
    "print(mdt.rmsd(traj, traj[0], atom_indices=traj.topology.select('protein')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make your workflow identify which snapshot from your trajectory has the highest RMSD from the starting structure, and then energy minimise that (this script may raise a warning from `distributed` - don't worry about that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy minimising snapshot 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/charlie/.local/lib/python3.9/site-packages/distributed/worker.py:4278: UserWarning: Large object of size 3.52 MiB detected in task graph: \n",
      "  ('em.mdp', <mdtraj.Trajectory with 1 frames, 20521 ... x7fd3b2ee7a00>)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'max_rmsd.gro'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rmsdlist = mdt.rmsd(traj, traj[0], atom_indices=traj.topology.select('protein'))\n",
    "i = np.argmax(rmsdlist)\n",
    "print('Energy minimising snapshot {}'.format(i))\n",
    "selected_snapshot = traj[i]\n",
    "em2_tprfile = client.submit(grompp2, emfile, selected_snapshot)\n",
    "em2_crds, em2_logfile = client.submit(mdrun, em2_tprfile)\n",
    "em2_crds.result().save('max_rmsd.gro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Putting it all together\n",
    "\n",
    "Here is a Python function that in effect does all the above: takes a set of starting coordinates, a topology file, and two .mdp files (one for an energy minimisation, one for an MD run), runs the workflow and then returns the energy-minimised structure of the snapshot with the highest RMSD from the starting structure. The function does everything, including creating the required tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy minimising snapshot 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'final_coordinates.gro'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_workflow(crd_filename, top_filename, em_mdp_filename, md_mdp_filename):\n",
    "    # Over to you!\n",
    "    # Load data:\n",
    "    \n",
    "    # Create tasks:\n",
    "    grompp = tasks.SubprocessTask('gmx grompp -f x.mdp -c x.gro -p x.top -o x.tpr -maxwarn 1')\n",
    "    grompp.set_inputs(['x.mdp', 'x.gro'])\n",
    "    grompp.set_constant('x.top', top_filename)\n",
    "    grompp.set_outputs(['x.tpr'])\n",
    "    \n",
    "    mdrun = tasks.SubprocessTask('gmx mdrun -s x.tpr -c x.gro -g x.log -e x.edr -o x.trr -ntmpi 1')\n",
    "    mdrun.set_inputs(['x.tpr'])\n",
    "    mdrun.set_outputs(['x.gro', 'x.log'])\n",
    "    \n",
    "    mdrun_with_traj = mdrun.copy()\n",
    "    mdrun_with_traj.set_outputs(['x.gro', 'x.log', 'x.trr'])\n",
    "    \n",
    "    # Run workflow (note nested client.submits() - compact but not neccessary!):\n",
    "    em_crds, em_logfile = client.submit(mdrun, client.submit(grompp, em_mdp_filename, crd_filename))\n",
    "    md_crds, md_logfile, md_traj = client.submit(mdrun_with_traj, client.submit(grompp, md_mdp_filename, em_crds))\n",
    "    traj = mdt.load(md_traj.result(), top=crd_filename)\n",
    "    rmsdlist = mdt.rmsd(traj, traj[0], atom_indices=traj.topology.select('protein'))\n",
    "    i = np.argmax(rmsdlist)\n",
    "    print('Energy minimising snapshot {}'.format(i))\n",
    "    em2_crds, em2_logfile = client.submit(mdrun, client.submit(grompp, em_mdp_filename, traj[i]))\n",
    "    \n",
    "    # Return final structure:\n",
    "    return em2_crds.result()\n",
    "\n",
    "# Test the workflow:\n",
    "final_crds = my_workflow('bpti.gro', 'bpti.top', 'em.mdp', 'nvt.mdp')\n",
    "final_crds.save('final_coordinates.gro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "taskspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
